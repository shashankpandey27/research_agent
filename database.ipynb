{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add all the dataframe to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env') \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('')\n",
    "\n",
    "## \n",
    "import openai\n",
    "# from openai.embeddings_utils import get_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = Path(os.environ.get('DATA_DIRECTORY', './'))\n",
    "ner_data_directory = data_directory/'named_entities'\n",
    "\n",
    "## The `DATA_DIRECTORY` above points to the `data` directory in the folloing repo\n",
    "## Data repo https://github.com/rahulnyk/mahabharata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "text_embedding_model = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Langchain Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "## For generating and persisting Embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=text_embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_localdb = True\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "SUPABASE_PASSWORD = os.environ['SUPABASE_PASSWORD']\n",
    "SUPABASE_DBUSER = os.environ['SUPABASE_DBUSER']\n",
    "SUPABASE_DATABASE = os.environ['SUPABASE_DATABASE']\n",
    "\n",
    "PGVECTOR_USER = os.environ['PGVECTOR_USER']\n",
    "PGVECTOR_PASSWORD = os.environ['PGVECTOR_PASSWORD']\n",
    "PGVECTOR_DATABASE = os.environ['PGVECTOR_DATABASE']\n",
    "\n",
    "localdb_string = f\"postgresql://{PGVECTOR_USER}:{PGVECTOR_PASSWORD}@localhost:5432/{PGVECTOR_DATABASE}\"\n",
    "supabasedb_string = f\"postgresql://{SUPABASE_DBUSER}:{SUPABASE_PASSWORD}@db.doxggeyqopdnxfhseufq.supabase.co:5432/{SUPABASE_DATABASE}\"\n",
    "\n",
    "connection_string = localdb_string if use_localdb else supabasedb_string\n",
    "\n",
    "engine = create_engine(connection_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df2db(dataframe, table_name, connection, if_exist = 'fail', chunksize = 4999):\n",
    "    \"\"\"\n",
    "        dataframe, table_name, connection, if_exist: default fail, chunksize: default 4999\n",
    "        chunksize - 4999, The max seems to be 5K rows at a time. \n",
    "        if_exist - 'fail', 'replace' or 'append'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = dataframe.to_sql(table_name, con=connection, if_exists = if_exist, chunksize=chunksize)\n",
    "        print(result)\n",
    "        return result\n",
    "    except Exception as E:\n",
    "        print(E, '- Request failed.')\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Glossary to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'character_glossary' already exists. - Request failed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_directory/\"tiny_tales_glossary.csv\", sep=\"|\")\n",
    "df2db(df, table_name = 'character_glossary', connection=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add named entities to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_table_name = 'named_entities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size (133328, 5)\n"
     ]
    }
   ],
   "source": [
    "file_list = glob.glob(f\"{ner_data_directory}/*_named_entities.csv\")\n",
    "\n",
    "dataframes = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file, sep=\"|\")\n",
    "    dataframes = dataframes + [df]\n",
    "\n",
    "dfne = pd.concat(dataframes)\n",
    "dfne.head()\n",
    "print(\"Dataframe size\", dfne.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'named_entities' already exists. - Request failed.\n"
     ]
    }
   ],
   "source": [
    "## Set if_exist to 'replace' for regenerating the database. \n",
    "result = df2db(\n",
    "    dfne, \n",
    "    table_name = ne_table_name, \n",
    "    connection=engine, \n",
    "    if_exist = 'fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select * from {ne_table_name} \n",
    "    where entity = 'PER' and \n",
    "    name = 'Abhimanyu'\n",
    "\"\"\"\n",
    "res = pd.read_sql(query, index_col='index', con = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>entity</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>count</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Abhimanyu</td>\n",
       "      <td>PER</td>\n",
       "      <td>cid_0ca1dc3d022343e3a2a5000a6db6ce99</td>\n",
       "      <td>1</td>\n",
       "      <td>km_ganguli_translation_5.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Abhimanyu</td>\n",
       "      <td>PER</td>\n",
       "      <td>cid_1d238a4680a64ecc93c4318fbeb68f4b</td>\n",
       "      <td>2</td>\n",
       "      <td>km_ganguli_translation_5.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Abhimanyu</td>\n",
       "      <td>PER</td>\n",
       "      <td>cid_248a6987461544718997d855ffcddd17</td>\n",
       "      <td>1</td>\n",
       "      <td>km_ganguli_translation_5.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Abhimanyu</td>\n",
       "      <td>PER</td>\n",
       "      <td>cid_45c0fba1423a4e55a6569dd164d9a824</td>\n",
       "      <td>1</td>\n",
       "      <td>km_ganguli_translation_5.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Abhimanyu</td>\n",
       "      <td>PER</td>\n",
       "      <td>cid_5087512a7e064ebc8d5ef799d724f0cb</td>\n",
       "      <td>1</td>\n",
       "      <td>km_ganguli_translation_5.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name entity                              chunk_id  count  \\\n",
       "index                                                                  \n",
       "54     Abhimanyu    PER  cid_0ca1dc3d022343e3a2a5000a6db6ce99      1   \n",
       "55     Abhimanyu    PER  cid_1d238a4680a64ecc93c4318fbeb68f4b      2   \n",
       "56     Abhimanyu    PER  cid_248a6987461544718997d855ffcddd17      1   \n",
       "57     Abhimanyu    PER  cid_45c0fba1423a4e55a6569dd164d9a824      1   \n",
       "58     Abhimanyu    PER  cid_5087512a7e064ebc8d5ef799d724f0cb      1   \n",
       "\n",
       "                               file  \n",
       "index                                \n",
       "54     km_ganguli_translation_5.csv  \n",
       "55     km_ganguli_translation_5.csv  \n",
       "56     km_ganguli_translation_5.csv  \n",
       "57     km_ganguli_translation_5.csv  \n",
       "58     km_ganguli_translation_5.csv  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually calculate embeddings. just for texting. \n",
    "## Ultimately I think it is wise to use the langchain vectorstore. \n",
    "\n",
    "# def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "#    text = text.replace(\"\\n\", \" \")\n",
    "#    return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiny Tales Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_tinytales = pd.read_csv(f\"{data_directory}/tiny_tales_summaries.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle Tilak Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kaggle Tilak dataframe\n",
    "\n",
    "df_kaggletilak = pd.read_csv(f\"{data_directory}/kaggle_tilak_summaries.csv\", sep=\"|\")\n",
    "df_kaggletilak = df_kaggletilak.replace(np.nan, '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load KM Ganguli Books Data\n",
    "\n",
    "kmgt = KM Ganguli translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size (9731, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_number</th>\n",
       "      <th>section</th>\n",
       "      <th>section_name</th>\n",
       "      <th>text</th>\n",
       "      <th>para_number</th>\n",
       "      <th>book_name</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Mahabharata\\n\\nof\\n\\nKrishna-Dwaipayana Vy...</td>\n",
       "      <td>1</td>\n",
       "      <td>Bhishma Parva</td>\n",
       "      <td>93</td>\n",
       "      <td>cid_59ceb7c3048f430a9d486b11745d2cf7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>SECTION I</td>\n",
       "      <td>Jamvu-khanda Nirmana Parva</td>\n",
       "      <td>\\n\\nOM! HAVING BOWED down to Narayana, and Nar...</td>\n",
       "      <td>1</td>\n",
       "      <td>Bhishma Parva</td>\n",
       "      <td>512</td>\n",
       "      <td>cid_275ad5c5673d44069c4b5be7494d418d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>SECTION I</td>\n",
       "      <td>Jamvu-khanda Nirmana Parva</td>\n",
       "      <td>\\n\"Beholding the standard-top of Pritha's son,...</td>\n",
       "      <td>2</td>\n",
       "      <td>Bhishma Parva</td>\n",
       "      <td>516</td>\n",
       "      <td>cid_b2f2fb019a3041f68a555203f1179686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>SECTION I</td>\n",
       "      <td>Jamvu-khanda Nirmana Parva</td>\n",
       "      <td>Those that left the ranks should never be slai...</td>\n",
       "      <td>3</td>\n",
       "      <td>Bhishma Parva</td>\n",
       "      <td>264</td>\n",
       "      <td>cid_56b6bcf84cbe4d81bd8cf6be496bd78d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>SECTION II</td>\n",
       "      <td></td>\n",
       "      <td>\\nVaisampayana said,--\"Seeing then the two arm...</td>\n",
       "      <td>1</td>\n",
       "      <td>Bhishma Parva</td>\n",
       "      <td>512</td>\n",
       "      <td>cid_24c46d25fe724028b0ec5b943e1cb0ec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_number     section                section_name  \\\n",
       "0            6                                           \n",
       "1            6   SECTION I  Jamvu-khanda Nirmana Parva   \n",
       "2            6   SECTION I  Jamvu-khanda Nirmana Parva   \n",
       "3            6   SECTION I  Jamvu-khanda Nirmana Parva   \n",
       "4            6  SECTION II                               \n",
       "\n",
       "                                                text  para_number  \\\n",
       "0  The Mahabharata\\n\\nof\\n\\nKrishna-Dwaipayana Vy...            1   \n",
       "1  \\n\\nOM! HAVING BOWED down to Narayana, and Nar...            1   \n",
       "2  \\n\"Beholding the standard-top of Pritha's son,...            2   \n",
       "3  Those that left the ranks should never be slai...            3   \n",
       "4  \\nVaisampayana said,--\"Seeing then the two arm...            1   \n",
       "\n",
       "       book_name  num_tokens                              chunk_id  \n",
       "0  Bhishma Parva          93  cid_59ceb7c3048f430a9d486b11745d2cf7  \n",
       "1  Bhishma Parva         512  cid_275ad5c5673d44069c4b5be7494d418d  \n",
       "2  Bhishma Parva         516  cid_b2f2fb019a3041f68a555203f1179686  \n",
       "3  Bhishma Parva         264  cid_56b6bcf84cbe4d81bd8cf6be496bd78d  \n",
       "4  Bhishma Parva         512  cid_24c46d25fe724028b0ec5b943e1cb0ec  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(f\"{data_directory}/km_ganguli_*.csv\")\n",
    "\n",
    "dataframes = []\n",
    "for file in file_list:\n",
    "    # print(\"File: \", file)\n",
    "    df = pd.read_csv(file, sep=\"|\")\n",
    "    dataframes = dataframes + [df]\n",
    "\n",
    "kmgt_dataframe = pd.concat(dataframes)\n",
    "kmgt_dataframe = kmgt_dataframe.replace(np.nan, '')\n",
    "kmgt_dataframe.head()\n",
    "print(\"Dataframe size\", kmgt_dataframe.shape)\n",
    "kmgt_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size (12307, 14)\n"
     ]
    }
   ],
   "source": [
    "## This is the combined  dataframe. but can be useful later. \n",
    "\n",
    "# file_list = glob.glob(f\"{data_directory}/*_summaries.csv\")\n",
    "\n",
    "# dataframes = []\n",
    "# for file in file_list:\n",
    "#     print(\"File: \", file)\n",
    "#     # df = pd.read_csv(file, sep=\"|\")\n",
    "#     # dataframes = dataframes + [df]\n",
    "\n",
    "dataframes = [kmgt_dataframe, df_kaggletilak, df_tinytales]\n",
    "\n",
    "combined_dataframe = pd.concat(dataframes)\n",
    "combined_dataframe = combined_dataframe.replace(np.nan, '')\n",
    "combined_dataframe.head()\n",
    "print(\"Dataframe size\", combined_dataframe.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a new function to save dataframe to vector store. \n",
    "The reason I am not using the default langchain `add_document` function is because it does not accept custom_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDataframeToVectorStore(dataframe, vector_store, text_col = 'text', id_col = 'chunk_id'):\n",
    "    metadatas = dataframe.drop([text_col], axis=1).to_dict('records')\n",
    "\n",
    "    result = vector_store.add_texts(\n",
    "        texts = dataframe[text_col], \n",
    "        ids=dataframe[id_col], \n",
    "        metadatas=metadatas,)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assert these flags if you want to recreate the tables\n",
    "recreate_tt_collection = False\n",
    "recreate_kt_collection = False\n",
    "recreate_kmgt_collection = False\n",
    "recreate_combined_collection = False\n",
    "\n",
    "if recreate_tt_collection:\n",
    "    tinytales_store = PGVector(\n",
    "        collection_name=\"tinytales\",\n",
    "        connection_string=connection_string,\n",
    "        embedding_function=embeddings,)\n",
    "    \n",
    "    addDataframeToVectorStore(df_tinytales, tinytales_store)\n",
    "\n",
    "if recreate_kt_collection:\n",
    "    kt_summary_store = PGVector(\n",
    "        collection_name=\"kt_summary\",\n",
    "        connection_string=connection_string,\n",
    "        embedding_function=embeddings,)\n",
    "    \n",
    "    addDataframeToVectorStore(df_kaggletilak, kt_summary_store)\n",
    "\n",
    "if recreate_kmgt_collection:\n",
    "    kmgt_books_store = PGVector(\n",
    "        collection_name='kmgt_books',\n",
    "        connection_string=connection_string,\n",
    "        embedding_function=embeddings,)\n",
    "    \n",
    "    addDataframeToVectorStore(kmgt_dataframe, kmgt_books_store)\n",
    "\n",
    "if recreate_combined_collection:\n",
    "    combined_store = PGVector(\n",
    "        collection_name='mahabharat_combined_text',\n",
    "        connection_string=connection_string,\n",
    "        embedding_function=embeddings,)\n",
    "    \n",
    "    addDataframeToVectorStore(combined_dataframe, combined_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Success! </b> \n",
    "    All the documents are saved in the vector store</div>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
